좋은 질문이에요 👍
최신 아키텍처 기법(특히 시험 범위 **AI 모델 개발 > 아키텍처 설계**)에는 **LoRA, NAS/DARTS** 외에도 많이 쓰이는 기법들이 있습니다.
제가 정리해드릴게요.

---

# 📌 최신 아키텍처 기법 정리

## 1. **LoRA (Low-Rank Adaptation)**

* 대형 모델 파라미터 전체를 업데이트하지 않고, 저차원 행렬을 추가해서 효율적 튜닝.
* 장점: 메모리/연산량 절약, 기존 모델 재사용.
* **출제됨 (파일럿 문항 10)**.

---

## 2. **NAS (Neural Architecture Search)**

* 신경망 구조를 자동으로 탐색하는 기법.
* **DARTS**: NAS의 대표적인 방법, 탐색 공간을 미분 가능하게 바꿔 Gradient Descent로 최적 구조 탐색.
* **출제됨 (파일럿 문항 21)**.

---

## 3. **MoE (Mixture of Experts)**

* 여러 개의 “전문가 네트워크(Experts)” 중 일부만 활성화하여 연산.
* LLM(예: Google Switch Transformer)에서 많이 사용.
* 장점: 모델 파라미터는 커도, 실제 연산은 일부 전문가만 사용 → 효율적.

---

## 4. **Adapter Layers**

* 사전학습된 모델에 “작은 추가 네트워크”를 삽입해 미세조정(Fine-tuning)하는 방법.
* LoRA와 비슷한 맥락(파라미터 효율적 튜닝).
* 장점: 원래 모델 파라미터는 동결, 추가된 Adapter만 학습 → 효율성 ↑.

---

## 5. **Prompt Tuning / Prefix Tuning**

* 대형 언어모델을 수정하지 않고, 입력 프롬프트나 임베딩 앞부분(prefix)만 학습.
* 장점: 모델 전체를 건드리지 않고도 새로운 태스크 적용 가능.
* 응용: NLP (텍스트 분류, QA).

---

## 6. **HyperNetworks**

* 네트워크의 파라미터를 다른 네트워크가 생성하도록 설계.
* 즉, “네트워크를 만드는 네트워크”.
* 장점: 다양한 태스크에 빠른 적응 가능.

---

## 7. **Neural ODE (Ordinary Differential Equation)**

* 신경망의 연산을 \*\*연속적 함수(ODE)\*\*로 정의.
* ResNet의 확장판으로 볼 수 있음.
* 장점: 메모리 효율적, 시간에 따른 연속적 데이터 모델링에 강점.

---

## 8. **EfficientNet (Compound Scaling)**

* 모델의 \*\*깊이(Depth), 너비(Width), 해상도(Resolution)\*\*를 균형 있게 확장.
* 단순히 레이어만 쌓는 것이 아니라, 최적의 비율로 조정.
* 장점: 같은 정확도에서 연산량(FLOPs) 최소화.
* **출제 가능성 ↑ (파일럿 문항 23은 FLOPs 관련 문제였음)**.

---

## 9. **Swin Transformer (Shifted Window Transformer)**

* Vision Transformer의 단점을 보완 → 이미지를 작은 “윈도우 단위”로 Self-Attention.
* 장점: 계산량 감소, 이미지 처리에 강력.
* 자율주행·영상 처리 시험에 나올 수 있음.

---

# ✅ 정리

| 기법                     | 특징                   | 시험 가능성          |
| ---------------------- | -------------------- | --------------- |
| LoRA                   | 파라미터 효율적 튜닝          | 이미 출제           |
| NAS / DARTS            | 자동 아키텍처 탐색           | 이미 출제           |
| MoE                    | 일부 전문가만 활성화          | 최신 트렌드          |
| Adapter Layers         | 추가 모듈만 학습            | LoRA와 함께 자주 언급  |
| Prompt / Prefix Tuning | 프롬프트 조정으로 튜닝         | LLM 관련          |
| HyperNetworks          | 네트워크를 만드는 네트워크       | 심화              |
| Neural ODE             | 연속적 모델링              | 시험 심화 가능        |
| EfficientNet           | Compound Scaling     | FLOPs 관련 문제와 연계 |
| Swin Transformer       | 이미지용 효율적 Transformer | 비전 응용 문제        |

---

👉 시험에서는 **LoRA, NAS/DARTS, EfficientNet, Adapter/Prompt Tuning** 정도가 출제 가능성이 높습니다.
심화라면 **MoE, Swin Transformer**까지 나올 수 있고요.

혹시 원하시면 제가 이 중에서 \*\*LoRA와 대비되는 다른 파라미터 효율적 튜닝 기법(Adapter, Prompt Tuning 등)\*\*만 묶어서 더 상세히 비교 설명해드릴까요?
