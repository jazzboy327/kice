μΆ‹μ€ μ§λ¬Έμ΄μ—μ” π‘
μµμ‹  μ•„ν‚¤ν…μ² κΈ°λ²•(νΉν μ‹ν— λ²”μ„ **AI λ¨λΈ κ°λ° > μ•„ν‚¤ν…μ² μ„¤κ³„**)μ—λ” **LoRA, NAS/DARTS** μ™Έμ—λ„ λ§μ΄ μ“°μ΄λ” κΈ°λ²•λ“¤μ΄ μμµλ‹λ‹¤.
μ κ°€ μ •λ¦¬ν•΄λ“λ¦΄κ²μ”.

---

# π“ μµμ‹  μ•„ν‚¤ν…μ² κΈ°λ²• μ •λ¦¬

## 1. **LoRA (Low-Rank Adaptation)**

* λ€ν• λ¨λΈ νλΌλ―Έν„° μ „μ²΄λ¥Ό μ—…λ°μ΄νΈν•μ§€ μ•κ³ , μ €μ°¨μ› ν–‰λ ¬μ„ μ¶”κ°€ν•΄μ„ ν¨μ¨μ  νλ‹.
* μ¥μ : λ©”λ¨λ¦¬/μ—°μ‚°λ‰ μ μ•½, κΈ°μ΅΄ λ¨λΈ μ¬μ‚¬μ©.
* **μ¶μ λ¨ (νμΌλΏ λ¬Έν•­ 10)**.

---

## 2. **NAS (Neural Architecture Search)**

* μ‹ κ²½λ§ κµ¬μ΅°λ¥Ό μλ™μΌλ΅ νƒμƒ‰ν•λ” κΈ°λ²•.
* **DARTS**: NASμ λ€ν‘μ μΈ λ°©λ²•, νƒμƒ‰ κ³µκ°„μ„ λ―Έλ¶„ κ°€λ¥ν•κ² λ°”κΏ” Gradient Descentλ΅ μµμ  κµ¬μ΅° νƒμƒ‰.
* **μ¶μ λ¨ (νμΌλΏ λ¬Έν•­ 21)**.

---

## 3. **MoE (Mixture of Experts)**

* μ—¬λ¬ κ°μ β€μ „λ¬Έκ°€ λ„¤νΈμ›ν¬(Experts)β€ μ¤‘ μΌλ¶€λ§ ν™μ„±ν™”ν•μ—¬ μ—°μ‚°.
* LLM(μ: Google Switch Transformer)μ—μ„ λ§μ΄ μ‚¬μ©.
* μ¥μ : λ¨λΈ νλΌλ―Έν„°λ” μ»¤λ„, μ‹¤μ  μ—°μ‚°μ€ μΌλ¶€ μ „λ¬Έκ°€λ§ μ‚¬μ© β†’ ν¨μ¨μ .

---

## 4. **Adapter Layers**

* μ‚¬μ „ν•™μµλ λ¨λΈμ— β€μ‘μ€ μ¶”κ°€ λ„¤νΈμ›ν¬β€λ¥Ό μ‚½μ…ν•΄ λ―Έμ„Έμ΅°μ •(Fine-tuning)ν•λ” λ°©λ²•.
* LoRAμ™€ λΉ„μ·ν• λ§¥λ½(νλΌλ―Έν„° ν¨μ¨μ  νλ‹).
* μ¥μ : μ›λ λ¨λΈ νλΌλ―Έν„°λ” λ™κ²°, μ¶”κ°€λ Adapterλ§ ν•™μµ β†’ ν¨μ¨μ„± β†‘.

---

## 5. **Prompt Tuning / Prefix Tuning**

* λ€ν• μ–Έμ–΄λ¨λΈμ„ μμ •ν•μ§€ μ•κ³ , μ…λ ¥ ν”„λ΅¬ν”„νΈλ‚ μ„λ² λ”© μ•λ¶€λ¶„(prefix)λ§ ν•™μµ.
* μ¥μ : λ¨λΈ μ „μ²΄λ¥Ό κ±΄λ“λ¦¬μ§€ μ•κ³ λ„ μƒλ΅μ΄ νƒμ¤ν¬ μ μ© κ°€λ¥.
* μ‘μ©: NLP (ν…μ¤νΈ λ¶„λ¥, QA).

---

## 6. **HyperNetworks**

* λ„¤νΈμ›ν¬μ νλΌλ―Έν„°λ¥Ό λ‹¤λ¥Έ λ„¤νΈμ›ν¬κ°€ μƒμ„±ν•λ„λ΅ μ„¤κ³„.
* μ¦‰, β€λ„¤νΈμ›ν¬λ¥Ό λ§λ“λ” λ„¤νΈμ›ν¬β€.
* μ¥μ : λ‹¤μ–‘ν• νƒμ¤ν¬μ— λΉ λ¥Έ μ μ‘ κ°€λ¥.

---

## 7. **Neural ODE (Ordinary Differential Equation)**

* μ‹ κ²½λ§μ μ—°μ‚°μ„ \*\*μ—°μ†μ  ν•¨μ(ODE)\*\*λ΅ μ •μ.
* ResNetμ ν™•μ¥νμΌλ΅ λ³Ό μ μμ.
* μ¥μ : λ©”λ¨λ¦¬ ν¨μ¨μ , μ‹κ°„μ— λ”°λ¥Έ μ—°μ†μ  λ°μ΄ν„° λ¨λΈλ§μ— κ°•μ .

---

## 8. **EfficientNet (Compound Scaling)**

* λ¨λΈμ \*\*κΉμ΄(Depth), λ„λΉ„(Width), ν•΄μƒλ„(Resolution)\*\*λ¥Ό κ· ν• μκ² ν™•μ¥.
* λ‹¨μν λ μ΄μ–΄λ§ μ“λ” κ²ƒμ΄ μ•„λ‹λΌ, μµμ μ λΉ„μ¨λ΅ μ΅°μ •.
* μ¥μ : κ°™μ€ μ •ν™•λ„μ—μ„ μ—°μ‚°λ‰(FLOPs) μµμ†ν™”.
* **μ¶μ  κ°€λ¥μ„± β†‘ (νμΌλΏ λ¬Έν•­ 23μ€ FLOPs κ΄€λ ¨ λ¬Έμ μ€μ)**.

---

## 9. **Swin Transformer (Shifted Window Transformer)**

* Vision Transformerμ λ‹¨μ μ„ λ³΄μ™„ β†’ μ΄λ―Έμ§€λ¥Ό μ‘μ€ β€μλ„μ° λ‹¨μ„β€λ΅ Self-Attention.
* μ¥μ : κ³„μ‚°λ‰ κ°μ†, μ΄λ―Έμ§€ μ²λ¦¬μ— κ°•λ ¥.
* μμ¨μ£Όν–‰Β·μμƒ μ²λ¦¬ μ‹ν—μ— λ‚μ¬ μ μμ.

---

# β… μ •λ¦¬

| κΈ°λ²•                     | νΉμ§•                   | μ‹ν— κ°€λ¥μ„±          |
| ---------------------- | -------------------- | --------------- |
| LoRA                   | νλΌλ―Έν„° ν¨μ¨μ  νλ‹          | μ΄λ―Έ μ¶μ            |
| NAS / DARTS            | μλ™ μ•„ν‚¤ν…μ² νƒμƒ‰           | μ΄λ―Έ μ¶μ            |
| MoE                    | μΌλ¶€ μ „λ¬Έκ°€λ§ ν™μ„±ν™”          | μµμ‹  νΈλ λ“          |
| Adapter Layers         | μ¶”κ°€ λ¨λ“λ§ ν•™μµ            | LoRAμ™€ ν•¨κ» μμ£Ό μ–ΈκΈ‰  |
| Prompt / Prefix Tuning | ν”„λ΅¬ν”„νΈ μ΅°μ •μΌλ΅ νλ‹         | LLM κ΄€λ ¨          |
| HyperNetworks          | λ„¤νΈμ›ν¬λ¥Ό λ§λ“λ” λ„¤νΈμ›ν¬       | μ‹¬ν™”              |
| Neural ODE             | μ—°μ†μ  λ¨λΈλ§              | μ‹ν— μ‹¬ν™” κ°€λ¥        |
| EfficientNet           | Compound Scaling     | FLOPs κ΄€λ ¨ λ¬Έμ μ™€ μ—°κ³„ |
| Swin Transformer       | μ΄λ―Έμ§€μ© ν¨μ¨μ  Transformer | λΉ„μ „ μ‘μ© λ¬Έμ         |

---

π‘‰ μ‹ν—μ—μ„λ” **LoRA, NAS/DARTS, EfficientNet, Adapter/Prompt Tuning** μ •λ„κ°€ μ¶μ  κ°€λ¥μ„±μ΄ λ†’μµλ‹λ‹¤.
μ‹¬ν™”λΌλ©΄ **MoE, Swin Transformer**κΉμ§€ λ‚μ¬ μ μκ³ μ”.

νΉμ‹ μ›ν•μ‹λ©΄ μ κ°€ μ΄ μ¤‘μ—μ„ \*\*LoRAμ™€ λ€λΉ„λλ” λ‹¤λ¥Έ νλΌλ―Έν„° ν¨μ¨μ  νλ‹ κΈ°λ²•(Adapter, Prompt Tuning λ“±)\*\*λ§ λ¬¶μ–΄μ„ λ” μƒμ„Έν λΉ„κµ μ„¤λ…ν•΄λ“λ¦΄κΉμ”?
