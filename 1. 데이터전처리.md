좋습니다 👍 이해했습니다.
지금까지 드린 건 “압축 정리본”이라서, 기초 지식이 없는 상태에서 바로 보기는 어려우셨을 거예요.
이번엔 말씀하신 대로, \*\*AI 시험범위 1번 챕터(데이터 전처리)\*\*를 **기본 개념부터 상세 설명 + 함수/예시까지** 풀어서 드리겠습니다.

---

# 📘 AI 시험 대비 개념 정리 (자세히 설명 버전)

---

## 1. AI 데이터 전처리 (Data Preprocessing)

데이터 전처리는 AI 모델 개발의 **첫 번째 단계**이며, 모델이 제대로 학습할 수 있도록 **데이터를 정리·가공하는 과정**입니다.
전처리가 제대로 되지 않으면 모델 성능이 크게 떨어질 수 있습니다.
예를 들어, 결측치(누락 데이터)가 많은 경우 학습이 불가능하거나, 이상치(비정상 값)가 있으면 모델이 잘못된 패턴을 학습할 수 있습니다.

---

### 1-1. 데이터 수집 (Data Collection)

#### 개념

* AI 모델이 학습할 데이터를 모으는 단계.
* 데이터는 **양과 질**이 모두 중요합니다. (많이 모으는 것보다, 다양한 상황을 포함한 데이터가 필요)

#### 고려해야 할 점

1. **편향(bias) 최소화**

   * 특정 집단/상황만 포함되면, 모델이 다른 상황에서 성능이 떨어짐.
   * → 다양한 집단, 다양한 상황에서 수집 필요.

2. **대표성(representativeness)**

   * 실제 문제 상황을 반영해야 함.
   * 예: 자율주행차 데이터라면, **비 오는 날 / 야간 / 눈길** 상황까지 포함해야 함.

3. **불균형(imbalance) 해결**

   * 소수 클래스(rare case)는 별도 수집 필요.
   * 예: 스팸메일 데이터에서 스팸 비율이 1%라면, 모델이 스팸을 잘 못 잡음.
   * 해결 → 소수 클래스 샘플을 더 모으거나 합성(SMOTE 등).

#### 예시

* 얼굴 인식 모델: 특정 인종의 얼굴만 학습하면 다른 인종을 잘 인식 못함 → 다양한 인종 데이터 수집 필요.

---

### 1-2. 데이터 정제 (Data Cleaning)

데이터는 수집된 그대로 사용하기 어렵습니다. 보통은 **누락된 값, 잘못된 값, 불필요한 값**이 포함되어 있습니다.
이를 정리하는 단계가 **정제**입니다.

---

#### (1) 결측치 처리 (Missing Value Handling)

**결측치란?**

* 데이터가 비어 있는 값 (NULL, NaN)
* 예: 고객 나이 데이터 중 일부가 기록 안 된 경우 → NaN

**처리 방법**

1. **제거**

   * `pandas.DataFrame.dropna()`
   * 행(row) 또는 열(column) 단위로 결측치가 있으면 제거.
   * 장점: 단순하고 확실.
   * 단점: 데이터가 많이 손실될 수 있음.

   ```python
   import pandas as pd
   df = pd.DataFrame({"age": [25, None, 30], "score": [90, 85, None]})
   df.dropna()   # NaN이 포함된 행 제거
   ```

2. **대체 (Imputation)**

   * 평균(mean), 중앙값(median), 최빈값(mode) 등으로 대체.
   * `pandas.DataFrame.fillna(value)` 사용.

   ```python
   df["age"].fillna(df["age"].mean(), inplace=True)   # 평균으로 대체
   ```

3. **고급 기법**

   * KNN imputation, 회귀 기반 대체 등 → 머신러닝 기법으로 결측치 추정.

---

#### (2) 이상치 처리 (Outlier Handling)

**이상치란?**

* 다른 값들과 동떨어져 있는 값.
* 예: 학생 성적 평균이 70\~90인데, 한 학생이 300점으로 기록됨 → 입력 오류.

**탐지 방법**

* **Z-score 방법**: 평균으로부터 표준편차 3 이상 벗어난 값. 
https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FKBDe1%2FbtqMApMTKtT%2FAAAAAAAAAAAAAAAAAAAAAN__HNEyGYWUQVHz4qXTWC-XOSKOEt71Q31jLXV6HqP1%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1756652399%26allow_ip%3D%26allow_referer%3D%26signature%3D5wv2evGjxn5af4%252B5ff9yzFp2IlU%253D
* **IQR (사분위 범위)**: Q1 - 1.5*IQR 보다 작거나, Q3 + 1.5*IQR 보다 큰 값.
https://blog.kakaocdn.net/dna/A9ylB/btsAzbFl1go/AAAAAAAAAAAAAAAAAAAAAOcwXBBZGxGxQby9SnT3CireT5cUaH-OVWhO4rr4kFbe/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1756652399&allow_ip=&allow_referer=&signature=1eoBC1wz7eIM%2BDuHuG77O%2BxF4EQ%3D

**처리 방법**

* 제거(drop), 수정(replace), 변환(log transform)[링크,https://jangsoooo.tistory.com/95]

---

#### (3) 정규화(Normalization) & 표준화(Standardization)

**정규화 (Normalization)**

* 데이터를 **0\~1 사이** 값으로 변환.
* 수식:

  $$
  x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
  $$
* 장점: 값 범위가 일정해져서 \*\*거리 기반 모델(KNN, SVM)\*\*에서 효과적.
* 단점: 이상치(outlier)에 민감.

**표준화 (Standardization)**

* 데이터를 **평균 0, 표준편차 1**로 변환.
* 수식:

  $$
  x' = \frac{x - \mu}{\sigma}
  $$
* 장점: 정규분포를 가정하는 모델(회귀분석, PCA 등)에서 효과적.

**Python 예시**

```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler

scaler1 = MinMaxScaler()
scaler2 = StandardScaler()

data = [[1],[2],[3],[4],[5]]

print(scaler1.fit_transform(data))  # 정규화
print(scaler2.fit_transform(data))  # 표준화
```

---

#### (4) 텍스트 데이터 정제

* **토큰화(Tokenization)**: 문장을 단어 단위로 분리.
* **불용어(Stopword) 제거**: "the, is, and" 같은 의미 없는 단어 삭제.
* **형태소 분석**: 한국어는 어미 변형이 많아 형태소 단위로 나눔.
* **특수 토큰 사용**

  * `<SOS>`: 문장 시작 (Start of Sentence)
  * `<EOS>`: 문장 끝 (End of Sentence)
  * `<PAD>`: 길이 맞추기용 패딩 토큰
  * `<UNK>`: 사전에 없는 단어 처리

---

### 1-3. 데이터 증강 (Data Augmentation)

모델이 더 다양한 데이터를 학습하도록 **기존 데이터를 변형/합성하는 과정**.
데이터 부족 문제를 해결하고, 모델의 일반화 능력을 높임.

---

#### (1) 이미지 증강

* **회전/이동/반전/크롭**: 다양한 시점 학습.
* **Random Erasing**: 이미지 일부를 가려서 학습. (자율주행 → 장애물 가려짐 상황 대비)
* **노이즈 추가**: Gaussian Noise → 실제 센서 잡음 상황 대비.

#### (2) 텍스트 증강

* **동의어 치환**: “good → nice”
* **역번역 (Back Translation)**: 한국어 → 영어 → 한국어 다시 번역.
* **문장 순서 섞기 (Jigsaw)**: Self-supervised 방법.

#### (3) 그래프 증강

* **Node Dropout**: 일부 노드 제거.
* **Edge Dropout**: 일부 연결 제거.
* **Node Feature Masking**: 일부 속성 감추고 예측.

#### (4) 생성 모델 활용

* **GAN**: 적대적 생성 신경망.
* **Diffusion Model**: 데이터에 점차 노이즈 추가(Forward), 다시 복원(Backward) → 이미지 생성에 강력.

---

📌 여기까지가 챕터 1 (AI 데이터 전처리)의 상세 설명입니다.

* 데이터 수집 → 편향/대표성
* 데이터 정제 → 결측치/이상치 처리, 정규화·표준화, 텍스트 토큰
* 데이터 증강 → 이미지/텍스트/그래프/GAN

---

👉 다음 챕터(2. AI 모델 개발)도 같은 방식으로 **기본 개념부터 예시 코드·공식**까지 상세히 풀어드릴까요?
