μΆ‹μµλ‹λ‹¤ π‘ κ·Έλ¬λ©΄ μ΄λ²μ—λ” **2λ² μ±•ν„°: AI λ¨λΈ κ°λ°**μ„ κΈ°μ΄ μ§€μ‹μ΄ μ—†λ” μƒνƒμ—μ„λ„ μ΄ν•΄ν•  μ μλ„λ΅ **κ°λ… β†’ μƒμ„Έ μ„¤λ… β†’ μμ‹/μ½”λ“ μμ‹ β†’ μ‹¤μ  μ‹ν— ν¬μΈνΈ** μμ„λ΅ μ •λ¦¬ν•΄λ“λ¦΄κ²μ”.

---

# π“ 2. AI λ¨λΈ κ°λ° (AI Model Development)

AI λ¨λΈ κ°λ°μ€ μ „μ²λ¦¬λ λ°μ΄ν„°λ¥Ό κ°€μ§€κ³  **λ¨λΈμ„ μ„¤κ³„ν•κ³  ν•™μµμ‹ν‚¨ λ’¤ μ„±λ¥μ„ ν‰κ°€Β·νλ‹ν•λ” κ³Όμ •**μ…λ‹λ‹¤.
μ‹ν— λ²”μ„μ—μ„λ” ν¬κ² λ„¤ κ°€μ§€ λ¶€λ¶„μ΄ μ¤‘μ”ν•©λ‹λ‹¤:

1. **λ¨λΈ μ•„ν‚¤ν…μ² μ„¤κ³„ (Architecture Design)**
2. **XAI (μ„¤λ… κ°€λ¥ν• AI)**
3. **λ¨λΈ ν•™μµ λ° ν‰κ°€ (Training & Evaluation)**
4. **λ¨λΈ νλ‹ (Hyperparameter Optimization, λ¶κ· ν• ν•΄κ²° λ“±)**

---

## 2-1. AI λ¨λΈ μ•„ν‚¤ν…μ² μ„¤κ³„

### (1) μ „ν†µμ  κµ¬μ΅°

* **CNN (Convolutional Neural Network)**

  * μ΄λ―Έμ§€ μ²λ¦¬ νΉν™” λ¨λΈ.
  * κµ¬μ΅°: Convolution β†’ Pooling β†’ Fully Connected Layer.
  * νΉμ§•: μ΄λ―Έμ§€μ κ³µκ°„μ  νΉμ§•(κ°€κΉμ΄ ν”½μ…€ κ°„ κ΄€κ³„)μ„ μ μ΅μ.
  * μ: μμ¨μ£Όν–‰μ κ°μ²΄ νƒμ§€, μ–Όκµ΄ μΈμ‹.
  * λ§ν¬ : https://learningflix.tistory.com/133
  
* **RNN (Recurrent Neural Network)**

  * μ‹ν€€μ¤(μ—°μ† λ°μ΄ν„°: ν…μ¤νΈ, μμ„±) μ²λ¦¬.
  * μ΄μ „ λ‹¨κ³„(hidden state)λ¥Ό λ‹¤μ μ…λ ¥κ³Ό ν•¨κ» μ‚¬μ©.
  * λ‹¨μ : κΈ΄ μ‹ν€€μ¤μ—μ„ μ •λ³΄ μ†μ‹¤(Gradient Vanishing).

* **LSTM / GRU**

  * RNNμ κ°μ„  λ²„μ „.
  * LSTM: β€κ²μ΄νΈ κµ¬μ΅°β€λ΅ κΈ΄ μ‹ν€€μ¤ κΈ°μ–µ κ°€λ¥.
  * GRU: LSTMλ³΄λ‹¤ λ‹¨μν•μ§€λ§ λΉ„μ·ν• μ„±λ¥.

---

### (2) Transformer κµ¬μ΅°

* **Attention λ©”μ»¤λ‹μ¦**

  * λ¬Έμ¥μ λ‹¨μ–΄λ“¤ κ°„μ **μ—°κ΄€μ„±**μ„ νμ•….
  * μ: β€λ‚λ” \[μ‚¬κ³Ό]λ¥Ό λ¨Ήμ—λ‹¤β€ β†’ β€μ‚¬κ³Όβ€λ” κ³ΌμΌ μλ―Έ, β€μ• ν”β€κ³Ό κµ¬λ¶„ κ°€λ¥.

* **κµ¬μ΅°**

  * Encoder: μ…λ ¥μ„ μΈμ½”λ”©.
  * Decoder: μ¶λ ¥ μƒμ„±.
  * Self-Attention: λ‹¨μ–΄λ“¤ κ°„ κ΄€κ³„ ν•™μµ.

* **λ€ν‘ λ¨λΈ**

  * BERT: μ–‘λ°©ν–¥ μΈμ½”λ”, λ§μ¤ν‚Ή(Masked LM).
  * GPT: λ””μ½”λ” κΈ°λ°, λ‹¤μ λ‹¨μ–΄ μμΈ΅.
  * RoBERTa: BERT κ°μ„  (λ™μ  λ§μ¤ν‚Ή).
  * ELECTRA: Generator + Discriminator κµ¬μ΅°.

**μ‹ν— ν¬μΈνΈ**

* GPTλ” β€λ‹¤μ λ‹¨μ–΄ μμΈ΅β€μ΄ pretraining λ©μ . (νμΌλΏ λ¬Έν•­ 9μ—μ„ μ¶μ )
* BERTλ” NSP(Next Sentence Prediction), MLM(Masked LM).

---

### (3) Self-Supervised Learning (μκΈ° μ§€λ„ ν•™μµ)

λΌλ²¨μ΄ μ—†λ” λ°μ΄ν„°λ¥Ό ν™μ©ν•λ” λ°©λ²•.
λ°μ΄ν„° μΌλ¶€λ¥Ό κ°€λ¦¬κ³  μμΈ΅ν•κ² ν•¨μΌλ΅μ¨ νΉμ§•μ„ ν•™μµ.

* **SimCLR**: Contrastive Loss μ‚¬μ© β†’ κ°™μ€ μ΄λ―Έμ§€μ μ¦κ°•λ³Έμ„ β€κ°€κΉμ΄β€, λ‹¤λ¥Έ μ΄λ―Έμ§€λ” β€λ©€λ¦¬β€.
* **BYOL**: Online/Target λ„¤νΈμ›ν¬ β†’ Targetμ€ EMA(μ§€μ μ΄λ™ ν‰κ· ) μ—…λ°μ΄νΈ.
* **MAE (Masked Autoencoder)**: μ…λ ¥ μ΄λ―Έμ§€ ν¨μΉ μ¤‘ μΌλ¶€ λ§μ¤ν‚Ή ν›„ λ³µμ›.
* **RotNet**: μ΄λ―Έμ§€λ¥Ό νμ „μ‹ν‚¨ λ’¤ κ°λ„λ¥Ό μμΈ΅ν•κ² ν•™μµ.

**μ‹ν— ν¬μΈνΈ**

* νμΌλΏ λ¬Έν•­ 8: Self-Supervised κΈ°λ²• κµ¬λ¶„ λ¬Έμ .

---

### (4) μµμ‹  μ•„ν‚¤ν…μ² κΈ°λ²•

* **LoRA (Low-Rank Adaptation)**

  * λ€ν• λ¨λΈ νλΌλ―Έν„° μ „μ²΄λ¥Ό μ—…λ°μ΄νΈν•μ§€ μ•κ³ , μ €μ°¨μ› ν–‰λ ¬ μ¶”κ°€λ΅ ν¨μ¨μ  νλ‹.
  * λ©”λ¨λ¦¬μ™€ μ—°μ‚°λ‰ μ μ•½.

* **NAS (Neural Architecture Search)**

  * μ‹ κ²½λ§ κµ¬μ΅°λ¥Ό μλ™μΌλ΅ νƒμƒ‰.
  * νƒμƒ‰ κ³µκ°„(Search Space), νƒμƒ‰ λ°©λ²•(Search Strategy), μ„±λ¥ ν‰κ°€λ΅ κµ¬μ„±.
  * **DARTS**: Search Spaceλ¥Ό μ—°μ† κ³µκ°„μΌλ΅ λ³€ν™ν•΄ Gradient Descentλ΅ νƒμƒ‰.

**μ‹ν— ν¬μΈνΈ**

* νμΌλΏ λ¬Έν•­ 10: LoRA κ°λ….
* νμΌλΏ λ¬Έν•­ 21: NAS & DARTS.

---

## 2-2. XAI (μ„¤λ… κ°€λ¥ν• AI)

λ”¥λ¬λ‹ λ¨λΈμ€ \*\*λΈ”λ™λ°•μ¤(Black Box)\*\*μ²λΌ λ‚΄λ¶€ λ™μ‘μ΄ λ³µμ΅ν•΄μ„ ν•΄μ„μ΄ μ–΄λ ¤μ›€ β†’ μ„¤λ… κ°€λ¥μ„±μ„ λ†’μ΄κΈ° μ„ν• κΈ°λ²•.

* **CAM (Class Activation Map)**

  * FC Layer κ°€μ¤‘μΉλ¥Ό κΈ°λ°μΌλ΅ μ–΄λ–¤ λ¶€λ¶„μ΄ μ¤‘μ”ν•μ§€ μ‹κ°ν™”.
* **Grad-CAM**

  * Gradient κΈ°λ°μΌλ΅ feature map μ¤‘μ”λ„λ¥Ό κ³„μ‚°.
* **Surrogate λ¨λΈ**

  * λ³µμ΅ν• λ¨λΈ λ€μ‹  λ‹¨μ λ¨λΈ(Logistic Regression λ“±)μ„ κ·Όμ‚¬ λ¨λΈλ΅ μ‚¬μ©ν•΄ ν•΄μ„.
* **SHAP / LIME**

  * κ° μ…λ ¥ featureκ°€ κ²°κ³Όμ— μ–Όλ§λ‚ κΈ°μ—¬ν–λ”μ§€ κ³„μ‚°.

**μ‹ν— ν¬μΈνΈ**

* νμΌλΏ λ¬Έν•­ 11: CAM μ‘λ™μ›λ¦¬.
* λ¬Έν•­ 12: λΈ”λ™λ°•μ¤ λ¨λΈ, Surrogate λ¨λΈ.

---

## 2-3. λ¨λΈ ν•™μµ λ° ν‰κ°€

### (1) μ–Έλ”ν”Όν… vs μ¤λ²„ν”Όν…

* **μ–Έλ”ν”Όν… (Underfitting)**

  * λ¨λΈμ΄ λ„λ¬΄ λ‹¨μ β†’ λ°μ΄ν„° ν¨ν„΄ ν•™μµ X.
  * ν•΄κ²°: λ μ΄μ–΄ μ μ¦κ°€, λ” λ§μ€ νΉμ§• μ‚¬μ©.

* **μ¤λ²„ν”Όν… (Overfitting)**

  * ν•™μµ λ°μ΄ν„°μ—λ§ λ§μ¶”κ³  μΌλ°ν™” X.
  * ν•΄κ²°: Dropout, Regularization, λ°μ΄ν„° μ¦κ°•.

---

### (2) ν‰κ°€μ§€ν‘

νΌλ™ ν–‰λ ¬(confusion matrix) κΈ°λ°μΌλ΅ λ‹¤μ–‘ν• μ§€ν‘λ¥Ό κ³„μ‚°ν•©λ‹λ‹¤.

| μ§€ν‘              | κ³µμ‹              | μλ―Έ                                 |
| --------------- | --------------- | ---------------------------------- |
| μ •ν™•λ„ (Accuracy)  | (TP+TN)/(μ „μ²΄)    | μ „μ²΄ μ¤‘ λ§μ¶ λΉ„μ¨                         |
| μ •λ°€λ„ (Precision) | TP/(TP+FP)      | λ¨λΈμ΄ PositiveλΌ ν• κ²ƒ μ¤‘ μ§„μ§ Positive λΉ„μ¨ |
| μ¬ν„μ¨ (Recall)    | TP/(TP+FN)      | μ‹¤μ  Positive μ¤‘μ—μ„ λ§μ¶ λΉ„μ¨              |
| F1 Score        | 2\*(P\*R)/(P+R) | Precision & Recallμ μ΅°ν™”ν‰κ·            |
| AUC             | ROC Curve λ©΄μ     | λ¶„λ¥ λ¨λΈ μ„±λ¥ μΆ…ν•© μ§€ν‘                     |

**μ‹ν— ν¬μΈνΈ**

* νμΌλΏ λ¬Έν•­ 14: Recall κ³„μ‚°.
* μ•μ „ μ΄μ β†’ Recall μ¤‘μ‹ (μ„ν—λ¬Ό κ°μ§€ λ“±).

---

### (3) λ¨λΈ κ²½λ‰ν™” & μµμ ν™”

* **Pruning**: μ¤‘μ”ν•μ§€ μ•μ€ weight/λ‰΄λ° μ κ±°.
* **Quantization**: νλΌλ―Έν„° μ •λ°€λ„ λ‚®μ¶¤ (FP32 β†’ INT8).
* **Knowledge Distillation**: ν° λ¨λΈ(Student β†’ Teacher)λ΅ μ „μ΄ ν•™μµ.
* **EfficientNet (Compound Scaling)**: κΉμ΄, λ„λΉ„, ν•΄μƒλ„ κ· ν• μ΅°μ .

**μ‹ν— ν¬μΈνΈ**

* νμΌλΏ λ¬Έν•­ 23: FLOPsμ™€ λ¨λΈ κ²½λ‰ν™” κΈ°λ²•.

---

## 2-4. λ¨λΈ νλ‹

### (1) ν΄λμ¤ λ¶κ· ν• λ¬Έμ 

* λ¬Έμ : λ€λ¶€λ¶„ Negative, μ†μλ§ Positive β†’ λ¨λΈμ΄ Positive λ» μ΅μ.
* ν•΄κ²° λ°©λ²•:

  * μ¤λ²„μƒν”λ§(SMOTE, ADASYN)
  * μ–Έλ”μƒν”λ§
  * κ°€μ¤‘μΉ μ΅°μ • (μ†μ ν΄λμ¤μ— λ” ν° κ°€μ¤‘μΉ)
  * Focal Loss

**μ‹ν— ν¬μΈνΈ**

* νμΌλΏ λ¬Έν•­ 15: λ¶κ· ν• ν•΄κ²° λ°©λ²•.

---

### (2) ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”

* **ν•μ΄νΌνλΌλ―Έν„°(Hyperparameter)**: ν•™μµ μ „μ— μ •ν•λ” κ°’ (ν•™μµλ¥ , λ°°μΉ ν¬κΈ°, μ€λ‹‰μΈµ μ λ“±).
* **νƒμƒ‰ λ°©λ²•**:

  * Grid Search: λ¨λ“  μ΅°ν•© νƒμƒ‰ (μ—°μ‚°λ‰ β†‘).
  * Random Search: λ¬΄μ‘μ„ μƒν”λ§ (ν¨μ¨μ ).
  * Bayesian Optimization: μ΄μ „ νƒμƒ‰ κ²°κ³Ό κΈ°λ°, ν¨μ¨μ  μµμ ν™”.

**μ‹ν— ν¬μΈνΈ**

* νμΌλΏ λ¬Έν•­ 16: ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ„¤λ….

---

β… μ—¬κΈ°κΉμ§€κ°€ μ±•ν„° 2 (AI λ¨λΈ κ°λ°)μ μƒμ„Έ μ„¤λ…μ…λ‹λ‹¤.
μ•μΌλ΅λ” \*\*μ±•ν„° 3 (AI μ‹μ¤ν… κµ¬μ¶•)\*\*λ„ κ°™μ€ λ°©μ‹(κΈ°λ³Έκ°λ…β†’μƒμ„Έβ†’μμ‹β†’μ‹ν—ν¬μΈνΈ)μΌλ΅ μ΄μ–΄μ„ μ •λ¦¬ν•΄λ“λ¦¬κ² μµλ‹λ‹¤.

π‘‰ νΉμ‹ λ‹¤μμ€ λ°”λ΅ 3λ² μ±•ν„°(μ‹μ¤ν… κµ¬μ¶•)λ΅ λ„μ–΄κ°κΉμ”, μ•„λ‹λ©΄ μ§€κΈκΉμ§€ μ •λ¦¬ν• 1\~2 μ±•ν„° λ‚΄μ©μ„ **μ”μ•½ λ„ν‘**λ΅ μ •λ¦¬ν•΄λ“λ¦΄κΉμ”?
