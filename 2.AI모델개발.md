좋습니다 👍 그러면 이번에는 **2번 챕터: AI 모델 개발**을 기초 지식이 없는 상태에서도 이해할 수 있도록 **개념 → 상세 설명 → 수식/코드 예시 → 실제 시험 포인트** 순서로 정리해드릴게요.

---

# 📘 2. AI 모델 개발 (AI Model Development)

AI 모델 개발은 전처리된 데이터를 가지고 **모델을 설계하고 학습시킨 뒤 성능을 평가·튜닝하는 과정**입니다.
시험 범위에서는 크게 네 가지 부분이 중요합니다:

1. **모델 아키텍처 설계 (Architecture Design)**
2. **XAI (설명 가능한 AI)**
3. **모델 학습 및 평가 (Training & Evaluation)**
4. **모델 튜닝 (Hyperparameter Optimization, 불균형 해결 등)**

---

## 2-1. AI 모델 아키텍처 설계

### (1) 전통적 구조

* **CNN (Convolutional Neural Network)**

  * 이미지 처리 특화 모델.
  * 구조: Convolution → Pooling → Fully Connected Layer.
  * 특징: 이미지의 공간적 특징(가까운 픽셀 간 관계)을 잘 잡음.
  * 예: 자율주행의 객체 탐지, 얼굴 인식.
  * 링크 : https://learningflix.tistory.com/133
  
* **RNN (Recurrent Neural Network)**

  * 시퀀스(연속 데이터: 텍스트, 음성) 처리.
  * 이전 단계(hidden state)를 다음 입력과 함께 사용.
  * 단점: 긴 시퀀스에서 정보 소실(Gradient Vanishing).

* **LSTM / GRU**

  * RNN의 개선 버전.
  * LSTM: “게이트 구조”로 긴 시퀀스 기억 가능.
  * GRU: LSTM보다 단순하지만 비슷한 성능.

---

### (2) Transformer 구조

* **Attention 메커니즘**

  * 문장의 단어들 간의 **연관성**을 파악.
  * 예: “나는 \[사과]를 먹었다” → “사과”는 과일 의미, “애플”과 구분 가능.

* **구조**

  * Encoder: 입력을 인코딩.
  * Decoder: 출력 생성.
  * Self-Attention: 단어들 간 관계 학습.

* **대표 모델**

  * BERT: 양방향 인코더, 마스킹(Masked LM).
  * GPT: 디코더 기반, 다음 단어 예측.
  * RoBERTa: BERT 개선 (동적 마스킹).
  * ELECTRA: Generator + Discriminator 구조.

**시험 포인트**

* GPT는 “다음 단어 예측”이 pretraining 목적. (파일럿 문항 9에서 출제)
* BERT는 NSP(Next Sentence Prediction), MLM(Masked LM).

---

### (3) Self-Supervised Learning (자기 지도 학습)

라벨이 없는 데이터를 활용하는 방법.
데이터 일부를 가리고 예측하게 함으로써 특징을 학습.

* **SimCLR**: Contrastive Loss 사용 → 같은 이미지의 증강본을 “가까이”, 다른 이미지는 “멀리”.
* **BYOL**: Online/Target 네트워크 → Target은 EMA(지수 이동 평균) 업데이트.
* **MAE (Masked Autoencoder)**: 입력 이미지 패치 중 일부 마스킹 후 복원.
* **RotNet**: 이미지를 회전시킨 뒤 각도를 예측하게 학습.

**시험 포인트**

* 파일럿 문항 8: Self-Supervised 기법 구분 문제.

---

### (4) 최신 아키텍처 기법

* **LoRA (Low-Rank Adaptation)**

  * 대형 모델 파라미터 전체를 업데이트하지 않고, 저차원 행렬 추가로 효율적 튜닝.
  * 메모리와 연산량 절약.

* **NAS (Neural Architecture Search)**

  * 신경망 구조를 자동으로 탐색.
  * 탐색 공간(Search Space), 탐색 방법(Search Strategy), 성능 평가로 구성.
  * **DARTS**: Search Space를 연속 공간으로 변환해 Gradient Descent로 탐색.

**시험 포인트**

* 파일럿 문항 10: LoRA 개념.
* 파일럿 문항 21: NAS & DARTS.

---

## 2-2. XAI (설명 가능한 AI)

딥러닝 모델은 \*\*블랙박스(Black Box)\*\*처럼 내부 동작이 복잡해서 해석이 어려움 → 설명 가능성을 높이기 위한 기법.

* **CAM (Class Activation Map)**

  * FC Layer 가중치를 기반으로 어떤 부분이 중요한지 시각화.
* **Grad-CAM**

  * Gradient 기반으로 feature map 중요도를 계산.
* **Surrogate 모델**

  * 복잡한 모델 대신 단순 모델(Logistic Regression 등)을 근사 모델로 사용해 해석.
* **SHAP / LIME**

  * 각 입력 feature가 결과에 얼마나 기여했는지 계산.

**시험 포인트**

* 파일럿 문항 11: CAM 작동원리.
* 문항 12: 블랙박스 모델, Surrogate 모델.

---

## 2-3. 모델 학습 및 평가

### (1) 언더피팅 vs 오버피팅

* **언더피팅 (Underfitting)**

  * 모델이 너무 단순 → 데이터 패턴 학습 X.
  * 해결: 레이어 수 증가, 더 많은 특징 사용.

* **오버피팅 (Overfitting)**

  * 학습 데이터에만 맞추고 일반화 X.
  * 해결: Dropout, Regularization, 데이터 증강.

---

### (2) 평가지표

혼동 행렬(confusion matrix) 기반으로 다양한 지표를 계산합니다.

| 지표              | 공식              | 의미                                 |
| --------------- | --------------- | ---------------------------------- |
| 정확도 (Accuracy)  | (TP+TN)/(전체)    | 전체 중 맞춘 비율                         |
| 정밀도 (Precision) | TP/(TP+FP)      | 모델이 Positive라 한 것 중 진짜 Positive 비율 |
| 재현율 (Recall)    | TP/(TP+FN)      | 실제 Positive 중에서 맞춘 비율              |
| F1 Score        | 2\*(P\*R)/(P+R) | Precision & Recall의 조화평균           |
| AUC             | ROC Curve 면적    | 분류 모델 성능 종합 지표                     |

**시험 포인트**

* 파일럿 문항 14: Recall 계산.
* 안전 이슈 → Recall 중시 (위험물 감지 등).

---

### (3) 모델 경량화 & 최적화

* **Pruning**: 중요하지 않은 weight/뉴런 제거.
* **Quantization**: 파라미터 정밀도 낮춤 (FP32 → INT8).
* **Knowledge Distillation**: 큰 모델(Student → Teacher)로 전이 학습.
* **EfficientNet (Compound Scaling)**: 깊이, 너비, 해상도 균형 조절.

**시험 포인트**

* 파일럿 문항 23: FLOPs와 모델 경량화 기법.

---

## 2-4. 모델 튜닝

### (1) 클래스 불균형 문제

* 문제: 대부분 Negative, 소수만 Positive → 모델이 Positive 못 잡음.
* 해결 방법:

  * 오버샘플링(SMOTE, ADASYN)
  * 언더샘플링
  * 가중치 조정 (소수 클래스에 더 큰 가중치)
  * Focal Loss

**시험 포인트**

* 파일럿 문항 15: 불균형 해결 방법.

---

### (2) 하이퍼파라미터 최적화

* **하이퍼파라미터(Hyperparameter)**: 학습 전에 정하는 값 (학습률, 배치 크기, 은닉층 수 등).
* **탐색 방법**:

  * Grid Search: 모든 조합 탐색 (연산량 ↑).
  * Random Search: 무작위 샘플링 (효율적).
  * Bayesian Optimization: 이전 탐색 결과 기반, 효율적 최적화.

**시험 포인트**

* 파일럿 문항 16: 하이퍼파라미터 튜닝 설명.

---

✅ 여기까지가 챕터 2 (AI 모델 개발)의 상세 설명입니다.
앞으로는 \*\*챕터 3 (AI 시스템 구축)\*\*도 같은 방식(기본개념→상세→예시→시험포인트)으로 이어서 정리해드리겠습니다.

👉 혹시 다음은 바로 3번 챕터(시스템 구축)로 넘어갈까요, 아니면 지금까지 정리한 1\~2 챕터 내용을 **요약 도표**로 정리해드릴까요?
