μΆ‹μµλ‹λ‹¤ π‘
κ·Έλ¬λ©΄ κµμ¬μ²λΌ **μ„μ ν• μ •λ¦¬(μ„¤λ… + μμ‹ ν¬ν•¨)** ν•νƒλ΅ μ‹ν—λ²”μ„μ— λ§μ¶°μ„ μ±•ν„°λ³„λ΅ ν•λ‚μ”© μ •λ¦¬ν•΄λ“λ¦΄κ²μ”.

---

# π“ AI μ‹ν— λ€λΉ„ κ°λ… μ •λ¦¬ (μ„μ ν•)

---

## 1. AI λ°μ΄ν„° μ „μ²λ¦¬

### 1-1. λ°μ΄ν„° μμ§‘ (Data Collection)

* **μ •μ**: AI λ¨λΈ ν•™μµμ— ν•„μ”ν• λ°μ΄ν„°λ¥Ό ν™•λ³΄ν•λ” λ‹¨κ³„.
* **μ¤‘μ”μ„±**: λ°μ΄ν„°μ λ€ν‘μ„±κ³Ό λ‹¤μ–‘μ„±μ΄ λ¶€μ΅±ν•λ©΄ **νΈν–¥(bias)** λ¬Έμ κ°€ λ°μƒ.
* **λ°©λ²•**

  * λ‹¤μ–‘ν• μ§‘λ‹¨μ—μ„ μμ§‘ν•μ—¬ **μ‚¬νμ  λ‹¤μ–‘μ„±** ν™•λ³΄.
  * **μ†μ ν΄λμ¤**(λ¶κ· ν• λ¬Έμ ) β†’ μ¶”κ°€ μμ§‘, μ¤λ²„μƒν”λ§.
  * **μ „λ¬Έκ°€ κ²€μ¦**: λ€ν‘μ„±μ„ ν‰κ°€ν•κ³  λ¶ν•„μ”ν• νΉμ„± μ κ±°.
* **μ£Όμμ **: νΉμ • κ°μΈ/μ§‘λ‹¨μ— μμ΅΄ν• λΌλ²¨λ§μ€ **νΈν–¥μ„ μ¦κ°€**μ‹ν‚΄.

**μμ‹ λ¬Έμ **:

* νμΌλΏ λ¬Έν•­ 1: λ°μ΄ν„° νΈν–¥ μ¤„μ΄λ” λ°©λ²•.
* λ‹µ: λΌλ²¨λ§μ„ ν• λ…μ΄ μ „λ‹΄ν•λ” κ²ƒμ€ νΈν–¥μ„ μ¤„μ΄μ§€ λ»ν•λ‹¤.

---

### 1-2. λ°μ΄ν„° μ •μ  (Data Cleaning)

* **κ²°μΈ΅μΉ μ²λ¦¬**: dropna(), fillna()
* **μ΄μƒμΉ μ²λ¦¬**: IQR, Z-score μ΄μ©ν•μ—¬ κ·Ήλ‹¨κ°’ μ κ±°/λ³΄μ •.
* **μ •κ·ν™”(Normalization)**

  * μμ‹: $x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$
  * λ²”μ„λ¥Ό \[0,1]λ΅ μ΅°μ •.
  * λ‹¨μ : μ΄μƒμΉμ— λ―Όκ°.
* **ν‘μ¤€ν™”(Standardization)**

  * μμ‹: $x' = \frac{x - \mu}{\sigma}$
  * ν‰κ·  0, ν‘μ¤€νΈμ°¨ 1λ΅ μ΅°μ •.
  * μ •κ·λ¶„ν¬ κ°€μ •, μ΄μƒμΉ μν–¥ μƒλ€μ μΌλ΅ μ μ.
* **ν…μ¤νΈ μ •μ **

  * Special Tokens: <SOS>, <EOS>, <PAD>, <UNK>

**μμ‹ λ¬Έμ **:

* νμΌλΏ λ¬Έν•­ 2: μ •κ·ν™” vs ν‘μ¤€ν™” μ°¨μ΄.
* νμΌλΏ λ¬Έν•­ 3: <SOS>, <EOS>, <PAD>, <UNK> κ°λ….
* νμΌλΏ λ¬Έν•­ 4: dropna(), round().astype(int).

---

### 1-3. λ°μ΄ν„° μ¦κ°• (Data Augmentation)

* **μ΄λ―Έμ§€ μ¦κ°•**: νμ „, λ°μ „, λ…Έμ΄μ¦ μ¶”κ°€, Random Erasing.
* **ν…μ¤νΈ μ¦κ°•**: λ‹¨μ–΄ μΉν™, μ—­λ²μ—­(back-translation).
* **κ·Έλν”„ μ¦κ°•**: Node Dropout, Edge Dropout, Feature Masking.
* **μƒμ„± λ¨λΈ ν™μ©**: GAN, Diffusion Model.

**μμ‹ λ¬Έμ **:

* νμΌλΏ λ¬Έν•­ 5: Semantic Segmentation β†’ Random Erasing.
* λ¬Έν•­ 6: κ·Έλν”„ μ¦κ°• β†’ Node Feature Masking.
* λ¬Έν•­ 7: Diffusion Model (Forward & Backward process).

---

## 2. AI λ¨λΈ κ°λ°

### 2-1. μ•„ν‚¤ν…μ² μ„¤κ³„

* **CNN**: μ΄λ―Έμ§€ μ²λ¦¬. Convolution β†’ Pooling β†’ Fully Connected.
* **RNN**: μμ°¨ λ°μ΄ν„° μ²λ¦¬. LSTM, GRU κ°μ„  λ²„μ „.
* **Transformer**: Attention κΈ°λ°. NLPΒ·λ©€ν‹°λ¨λ‹¬ ν•µμ‹¬ κµ¬μ΅°.
* **Self-Supervised Learning**:

  * SimCLR: Contrastive Loss.
  * BYOL: Online/Target λ„¤νΈμ›ν¬.
  * MAE: Masked Autoencoder.
  * RotNet: μ΄λ―Έμ§€ νμ „ κ°λ„ μμΈ΅.
* **λ€κ·λ¨ λ¨λΈ μµμ ν™”**

  * **LoRA**: μ €μ°¨μ› ν–‰λ ¬ λ¶„ν•΄λ΅ νλΌλ―Έν„° ν¨μ¨μ  ν•™μµ.
  * **NAS/DARTS**: μλ™ μ•„ν‚¤ν…μ² νƒμƒ‰ κΈ°λ²•.

**μμ‹ λ¬Έμ **:

* νμΌλΏ λ¬Έν•­ 8: Self-Supervised κΈ°λ²• κµ¬λ¶„.
* λ¬Έν•­ 9: NLP λ¨λΈ(GPT, BERT, RoBERTa, ELECTRA).
* λ¬Έν•­ 10: LoRA κ°λ….
* λ¬Έν•­ 21: NAS & DARTS.
* λ¬Έν•­ 22: ν•™μµκ³΅μ„  κΈ°λ° μ „λµ (κ³Όμ ν•©/κ³Όμ†μ ν•©).

---

### 2-2. XAI (μ„¤λ… κ°€λ¥ν• AI)

* **CAM**: Class Activation Map, FC Layer κ°€μ¤‘μΉ κΈ°λ°.
* **Grad-CAM**: Gradient κΈ°λ°, CNN νΉμ§•λ§µ μ‹κ°ν™”.
* **Surrogate λ¨λΈ**: λ³µμ΅ν• λΈ”λ™λ°•μ¤ λ¨λΈμ„ ν•΄μ„ν•κΈ° μ„ν•΄ λ‹¨μ λ¨λΈ(Logistic Regression λ“±)μ„ κ·Όμ‚¬ λ¨λΈλ΅ μ‚¬μ©.
* **SHAP / LIME**: Feature μ¤‘μ”λ„ κ³„μ‚°, λ¨λΈ ν•΄μ„.

**μμ‹ λ¬Έμ **:

* νμΌλΏ λ¬Έν•­ 11: CAM.
* λ¬Έν•­ 12: λΈ”λ™λ°•μ¤ λ¨λΈ, Surrogate.

---

### 2-3. λ¨λΈ ν•™μµ λ° ν‰κ°€

* **μ–Έλ”ν”Όν… vs μ¤λ²„ν”Όν…**

  * μ–Έλ”ν”Όν…: λ¨λΈ λ‹¨μ β†’ λ³µμ΅μ„± β†‘, νΉμ„± β†‘
  * μ¤λ²„ν”Όν…: λ¨λΈ κ³Όλ„ λ³µμ΅ β†’ Dropout, Regularization
* **ν‰κ°€μ§€ν‘**

  * Precision = TP / (TP+FP)
  * Recall = TP / (TP+FN)
  * F1 = 2 \* (Precision \* Recall) / (Precision+Recall)
  * AUC: ROC κ³΅μ„  λ©΄μ 
* **μ§€ν‘ μ„ νƒ**

  * μ¤νΈ ν•„ν„°: Recall β†‘
  * ν’μ§ κ²€μ‚¬(λ¶λ‰ν’ κ²€μ¶): F1 or Recall μ¤‘μ”

**μμ‹ λ¬Έμ **:

* νμΌλΏ λ¬Έν•­ 13: μ–Έλ”ν”Όν… ν•΄κ²° λ°©λ²•.
* λ¬Έν•­ 14: Recall κ³„μ‚°.
* λ¬Έν•­ 23: FLOPs & κ²½λ‰ν™” κΈ°λ²•.

---

### 2-4. λ¨λΈ νλ‹

* **ν΄λμ¤ λ¶κ· ν• ν•΄κ²°**

  * μ¤λ²„μƒν”λ§: SMOTE, ADASYN.
  * μ–Έλ”μƒν”λ§.
  * κ°€μ¤‘μΉ μ΅°μ • (Weighted Loss).
  * Focal Loss.
* **ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰**

  * Grid Search: μ „μ μ΅°μ‚¬.
  * Random Search: λ¬΄μ‘μ„ μƒν”λ§.
  * Bayesian Optimization: μ΄μ „ κ²°κ³Ό κΈ°λ° νƒμƒ‰.

**μμ‹ λ¬Έμ **:

* νμΌλΏ λ¬Έν•­ 15: ν΄λμ¤ λ¶κ· ν• ν•΄κ²° λ°©λ²•.
* λ¬Έν•­ 16: HPO κ΄€λ ¨ μ„¤λ….

---

## 3. AI μ‹μ¤ν… κµ¬μ¶•

### 3-1. μ‹μ¤ν… μ„¤κ³„ λ° λ°°ν¬

* **Model-in-service**: λ¨λΈμ„ μ›Ήμ„λΉ„μ¤ λ‚΄λ¶€μ— ν¬ν•¨. (κΈ°μ΅΄ μΈν”„λΌ ν™μ© O, ν™•μ¥μ„± β†“)
* **Model-as-service**: λ¨λΈμ„ λ…λ¦½ μ„λΉ„μ¤λ΅ λ¶„λ¦¬. (ν™•μ¥μ„± β†‘, μ§€μ—°μ‹κ°„ β†‘)

### 3-2. MLOps

* **Level 0**: μλ™, λ°λ³µ μ‘μ—….
* **Level 1**: ML νμ΄ν”„λΌμΈ μλ™ν™”.
* **Level 2**: CI/CD + λ¨λ‹ν„°λ§ μ™„μ „ μλ™ν™”.
* **λ¬Έμ  ν•΄κ²°**: λ°μ΄ν„°/λ¨λΈ λ“λ¦¬ν”„νΈ νƒμ§€, μλ™ μ¬ν•™μµ.

**μμ‹ λ¬Έμ **:

* νμΌλΏ λ¬Έν•­ 17: Model-in vs Model-as-service.
* λ¬Έν•­ 18: AI κ°λ°/λ°°ν¬ ν”„λ΅μ„Έμ¤.
* λ¬Έν•­ 24: MLOps μλ™ν™” μμ¤€.

---

## 4. AI νΈλ λ“

### 4-1. ν•™μµ ν¨λ¬λ‹¤μ„

* **Zero-shot Learning**: λ³Έ μ  μ—†λ” ν΄λμ¤ μμΈ΅.
* **Few-shot Learning**: μ μ€ μƒν”λ΅ ν•™μµ.
* **Generalized Zero-shot**: λ³Έ μ  μλ” + μ—†λ” ν΄λμ¤ λ¨λ‘ μΈμ‹.
* λ§ν¬ : https://velog.io/@euisuk-chung/%EC%83%9D%EC%84%B1-AI%EC%9D%98-%ED%95%99%EC%8A%B5-%EB%B0%A9%EC%8B%9D-%EC%A0%9C%EB%A1%9C%EC%83%B7%EC%9B%90%EC%83%B7%ED%93%A8%EC%83%B7-%EB%9F%AC%EB%8B%9D

### 4-2. LLM κΈ°λ²•

* **Chain-of-Thought Prompting**: λ‹¨κ³„μ  μ¶”λ΅  μ λ„.
* **RAG (Retrieval-Augmented Generation)**: κ²€μƒ‰ + μƒμ„± κ²°ν•©.

### 4-3. Responsible AI

* **Fairness, Transparency, Robustness, Privacy**

**μμ‹ λ¬Έμ **:

* νμΌλΏ λ¬Έν•­ 19: Zero-shot Learning.
* λ¬Έν•­ 20: Chain-of-Thought.

---

β… μ΄λ ‡κ² μ‹ν—λ²”μ„ μ „μ²΄λ¥Ό **κµμ¬ν• μ”μ•½**μΌλ΅ μ •λ¦¬ν•  μ μμµλ‹λ‹¤.
μ κ°€ μ΄μ–΄μ„ κ° μ±•ν„° λμ— \*\*μμƒ λ¬Έμ (μ—°μµμ©)\*\*λ„ λ„£μ–΄λ“λ¦΄ μ μλ”λ°μ”, νΉμ‹ μ›ν•μ‹λ©΄ **μ±•ν„°λ³„ μμƒλ¬Έμ  + ν•΄μ„¤**μ„ μ¶”κ°€ν•΄λ“λ¦΄κΉμ”?
